% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/S3functions.R
\name{algorithmsList}
\alias{algorithmsList}
\title{algorithmsList}
\usage{
algorithmsList(isClassification = NA, randomForest = TRUE, zeroR = FALSE,
  xgBoost = FALSE, GBM = FALSE, rpart = FALSE, lassoGlmnet = FALSE,
  ridgeGlmnet = FALSE, linearRegression = FALSE, linearEnsemble = FALSE,
  naiveBayes.weka = FALSE, bagging.weka = FALSE,
  votedPerceptron.weka = FALSE, classificationViaClustering.weka = FALSE,
  classificationViaRegression.weka = FALSE, randomSubSpace.weka = FALSE,
  bayesNet.weka = FALSE, libSVM.weka = FALSE, adaBoostM1.weka = FALSE,
  decisionTable.weka = FALSE, smo.weka = FALSE, repTree.weka = FALSE,
  smo.regression.weka = FALSE, regressionByDiscretization.weka = FALSE)
}
\arguments{
\item{isClassification}{A boolean indicator for whether to use only classification algorithms or only regression algorithms. If is NA the relevant algorithms are selected based on the target type and cardinality using the learning process.}

\item{randomForest}{random forest algorithm from the randomForest package.}

\item{zeroR}{predicts the mean (for a numeric class) or the mode (for a nominal class). This method requires almost no computation and will be the fastest to apply.}

\item{xgBoost}{eXtreme gradient boosted machine algorithm from the xgBoost package.  (currently does not support multiclass)}

\item{GBM}{gradient boosted machine algorithm from the xgBoost package.}

\item{rpart}{decision tree algorithm from the rpart package.}

\item{lassoGlmnet}{logistic regression / linear regression algorithm with alpha = 0 from the glmnet package.}

\item{ridgeGlmnet}{logistic regression / linear regression algorithm with alpha = 1 from the glmnet package.}

\item{linearRegression}{linear regression algorithm. lm from the base package.}

\item{linearEnsemble}{A linear ensemble of a collection of GBM and rpart algorithms.}

\item{naiveBayes.weka}{Classification algorithm. A naive Bayes classifier is a simple probabilistic classifier based on applying Bayes' theorem with strong independence assumptions.}

\item{bagging.weka}{bagging a classifier to reduce variance Can do classification and regression.}

\item{votedPerceptron.weka}{Classification algorithm. The perceptron is an algorithm for supervised classification of an input into one of several possible non-binary outputs.}

\item{classificationViaClustering.weka}{A simple meta-classifier that uses a clusterer for classification. For cluster algorithms that use a fixed number of clusterers.}

\item{classificationViaRegression.weka}{Doing classification using regression methods. Class is binarized and one regression model is built for each class value.}

\item{randomSubSpace.weka}{Constructs a decision tree based classifier that maintains highest accuracy on training data and improves on generalization accuracy as it grows in complexity. The classifier consists of multiple trees constructed systematically by pseudorandomly selecting subsets of components of the feature vector, that is, trees constructed in randomly chosen subspaces.}

\item{bayesNet.weka}{A Bayes Network classifier. Provides datastructures (network structure, conditional probability distributions, etc.) and facilities common to Bayes Network learning algorithms like K2 and B.}

\item{libSVM.weka}{Wrapper for the libSVM library, an integrated software for support vector classification, (C-SVC, nu-SVC), regression (epsilon-SVR, nu-SVR) and distribution estimation (one-class SVM). It supports multi-class classification.}

\item{adaBoostM1.weka}{Algorithm for boosting a nominal class classifier using the Adaboost M1 method. Only nominal class problems can be tackled.}

\item{decisionTable.weka}{Algorithm for building and using a simple decision table majority classifier.}

\item{smo.weka}{Implements John Platt's sequential minimal optimization algorithm for training a support vector classifier.}

\item{repTree.weka}{Fast decision tree learner. Builds a decision/regression tree using information gain/variance and prunes it using reduced-error pruning (with backfitting).}

\item{smo.regression.weka}{SMOreg implements the support vector machine for regression.A regression scheme that employs any classifier on a copy of the data that has the class attribute (equal-width) discretized.}

\item{regressionByDiscretization.weka}{A regression scheme that employs any classifier on a copy of the data that has the class attribute (equal-width) discretized.}
}
\description{
algorithmsList
}

